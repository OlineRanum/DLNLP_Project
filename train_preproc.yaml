# XNMT supports various ways to preprocess data as demonstrated in this example.
# Note that some preprocessing functionality relies on third-party tools.
!Experiment
  name: model_preprocessed_es_epoch100
  exp_global: !ExpGlobal
    # define some named strings that can be used throughout the experiment config:
    placeholders:
      DATA_IN: src/preprocess/preprocessed_data/en_es
      DATA_OUT: output/en_es
      SRC_LAN: es
      TRG_LAN: en
  model: !DefaultTranslator
    src_reader: !PlainTextReader
      vocab: !Vocab
        vocab_file: '{DATA_IN}/train.vocab.{SRC_LAN}'
    trg_reader: !PlainTextReader
      vocab: !Vocab
        vocab_file: '{DATA_IN}/train.vocab.{TRG_LAN}'
    src_embedder: !SimpleWordEmbedder
      emb_dim: 300
    encoder: !BiLSTMSeqTransducer
      layers: 1
    attender: !MlpAttender
      hidden_dim: 128
      state_dim: 128
      input_dim: 128
    decoder: !AutoRegressiveDecoder
      embedder: !SimpleWordEmbedder
        emb_dim: 128
      rnn: !UniLSTMSeqTransducer
        layers: 1
      transform: !AuxNonLinear
        output_dim: 300
      bridge: !NoBridge {}
    inference: !AutoRegressiveInference
      post_process: join-bpe
  train: !SimpleTrainingRegimen
    trainer: !AdamTrainer
      alpha: 0.0002
    batcher: !WordSrcBatcher
      avg_batch_size: 32
    lr_decay: 0.5
    run_for_epochs: 100
    patience: 500
    initial_patience: 500
    src_file: '{DATA_IN}/train.tok.norm.{SRC_LAN}'
    trg_file: '{DATA_IN}/train.tok.norm.{TRG_LAN}'
    dev_tasks:
      - !AccuracyEvalTask
        eval_metrics: bleu
        src_file: '{DATA_IN}/dev.tok.norm.{SRC_LAN}'
        ref_file: '{DATA_IN}/dev.norm.{TRG_LAN}'
        hyp_file: '{DATA_OUT}/{EXP}.dev_hyp'
      - !LossEvalTask
        src_file: '{DATA_IN}/dev.tok.norm.{SRC_LAN}'
        ref_file: '{DATA_IN}/dev.tok.norm.{TRG_LAN}'
  evaluate:
  - !AccuracyEvalTask
    eval_metrics: bleu
    src_file: '{DATA_IN}/test.tok.norm.{SRC_LAN}'
    ref_file: '{DATA_IN}/test.norm.{TRG_LAN}'
    hyp_file: '{DATA_OUT}/{EXP}.test_hyp'