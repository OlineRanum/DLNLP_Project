{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_lines(filename):\n",
    "    \"\"\"\n",
    "    Count the number of lines in a file.\n",
    "\n",
    "    :param filename: Name of the file to count lines in.\n",
    "    :return: Number of lines in the file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        line_count = sum(1 for _ in file)\n",
    "    return line_count\n",
    "\n",
    "def merge_n_lines_from_files(files_list, n, output_filename):\n",
    "    \"\"\"\n",
    "    Reads the first n lines from each file in files_list and writes them to output_filename.\n",
    "    \n",
    "    :param files_list: List of filenames to read from.\n",
    "    :param n: Number of lines to read from each file.\n",
    "    :param output_filename: Name of the output file to write lines to.\n",
    "    \"\"\"\n",
    "    i = 0 \n",
    "    with open(output_filename, 'w') as outfile:\n",
    "        for filename in files_list:\n",
    "            try:\n",
    "                with open(filename, 'r') as infile:\n",
    "                    # Read the first n lines\n",
    "                    lines = [next(infile) for _ in range(n[i])]\n",
    "                    outfile.writelines(lines)\n",
    "                    i += 1\n",
    "                    \n",
    "                    # Optionally, add a separator between data from different files\n",
    "                    #outfile.write(\"\\n\")\n",
    "            except StopIteration:\n",
    "                print(f\"{filename} had less than {n[i]} lines.\")\n",
    "                n_lines = count_lines(filename)\n",
    "                print('used ', n_lines, ' lines')\n",
    "                with open(filename, 'r') as infile:\n",
    "                    # Read the first n lines|\n",
    "                    lines = [next(infile) for _ in range(n_lines)]\n",
    "                    outfile.writelines(lines)\n",
    "                    i += 1\n",
    "            except FileNotFoundError:\n",
    "                print(f\"{filename} not found.\")\n",
    "\n",
    "def build_shared_vocab(input_files, output_filename):\n",
    "    \"\"\"\n",
    "    Concatenates the contents of multiple input files into a single output file.\n",
    "\n",
    "    :param input_files: List of filenames to read from.\n",
    "    :param output_filename: Name of the output file to write lines to.\n",
    "    \"\"\"\n",
    "    with open(output_filename, 'w') as outfile:\n",
    "        for filename in input_files:\n",
    "            try:\n",
    "                with open(filename, 'r') as infile:\n",
    "                    # Read and write lines to the output file\n",
    "                    outfile.writelines(infile.readlines())\n",
    "                    \n",
    "                    # Optionally, add a separator between data from different files\n",
    "                    outfile.write(\"\\n\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"{filename} not found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# INPUTS\n",
    "####################\n",
    "\n",
    "# SET LANGUAGES\n",
    "languages = ['be', 'hu', 'fa']\n",
    "l1, l2 = languages\n",
    "\n",
    "# SET number of sentences extracted per language for training\n",
    "n_points = [4500, 10000, 10000]\n",
    "\n",
    "# Set number of sentences extracted per language for dev\n",
    "n_points_dev = [450, 1000, 1000]\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "# Build data\n",
    "####################\n",
    "\n",
    "# Build source data\n",
    "files_list_train = []\n",
    "files_list_dev  = []\n",
    "\n",
    "for lan in languages:\n",
    "    files_list_train.append('en_' + lan + '/train.tok.norm.' + lan)\n",
    "    files_list_dev.append('en_' + lan + '/dev.tok.norm.' + lan)\n",
    "\n",
    "merge_n_lines_from_files(files_list_train, n_points, f'merged_files/{l1}_{l2}/merged_src_train.txt')\n",
    "merge_n_lines_from_files(files_list_dev, n_points_dev, f'merged_files/{l1}_{l2}/merged_src_dev.txt')\n",
    "\n",
    "\n",
    "# Build target data\n",
    "files_list_train = []\n",
    "files_list_dev  = []\n",
    "\n",
    "for lan in languages:\n",
    "    files_list_train.append('en_' + lan + '/train.tok.norm.en')\n",
    "    files_list_dev.append('en_' + lan + '/dev.tok.norm.en')\n",
    "\n",
    "merge_n_lines_from_files(files_list_train, n_points, f'merged_files/{l1}_{l2}/merged_tar_train.txt')\n",
    "merge_n_lines_from_files(files_list_dev, n_points_dev, f'merged_files/{l1}_{l2}/merged_tar_dev.txt')\n",
    "\n",
    "# Build Vocabularies\n",
    "# Build src vocab\n",
    "files_list_vocab  = []\n",
    "for lan in languages:\n",
    "    files_list_vocab.append('en_' + lan + '/train.vocab.' + lan)\n",
    "build_shared_vocab(files_list_vocab, f'merged_files/{l1}_{l2}/src_vocab.txt')\n",
    "\n",
    "\n",
    "# Build trg vocab\n",
    "files_list_vocab  = []\n",
    "for lan in languages:\n",
    "    files_list_vocab.append('en_' + lan + '/train.vocab.en')\n",
    "build_shared_vocab(files_list_vocab, f'merged_files/{l1}_{l2}/trg_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
